{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Pelé\n",
    "- Diego Maradonna\n",
    "- Johan Cruyff\n",
    "- Roberto Carlos\n",
    "- Franz Beckenbaur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal with this project is to optimize the playing strategy of the arcade video game Pac-Man using AI algorithms, in which the strategy aims for the number of points to be maximized and ghost encounters/lives lost to be minimized. For our implementation we will be using variables relating to the current game state, such as presence/location of ghosts (ghost encounters will remove one of your lives), point pellets(collecting them gives you points), power ups (allow you to do various things like eat ghosts and will reward you with points if you do so), and current location. These features will be used to train and test the various AI algorithms we plan on implementing with the goal of finding the one that optimizes its strategy the best. Performance/ success of each algorithm will be measured by how effectively it consistently maximizes the number of points, while minimizing the lives lost, in the shortest time possible. The models will be trained by extensive simulated gameplay and compared to each other.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Pac-man, originally called Puck-man in Japan, is a 1980 maze video game<a name =\"pacmanhistory\"></a>[<sup>[1]</sup>](#pacmannote) developed and published by Namco for arcades. The player controls Pac-Man, who must eat all the dots inside an enclosed mae while avoiding four colored ghosts – Blinky(red), Pinky(pink), Inky(cyan), and Clyde(orange) – who pursue Pac-Man. When Pac-Man eats all of the dots, the player advances to the next level. Levels are indicated by fruit icons at the bottom of the screen. In between levels are short cutscenes featuring Pac-Man and Blinky in humorous, comical situations. If a ghost catches Pac-Man, he loses a life; the game ends when all lives are lost. Each of the four ghosts has its own unique artificial intelligence (A.I.), or \"personality\": Blinky gives direct chase to Pac-Man; Pinky and Inky try to position themselves in front of Pac-Man, usually by cornering him; and Clyde switches between chasing Pac-Man and fleeing from him.<a name=\"pacmanarchive\"></a>[<sup>[2]</sup>](#pacmannote2)\n",
    "\n",
    "In this project, we will explore AI techniques for controlling ghost behaviors using rule-based systems and finite-state machines<a name=\"technologyandintelligence\"></a>[<sup>[3]</sup>](#technote). Additionally, we will implement Monte Carlo Tree Search (MCTS) to optimize Pac-Man’s movement based on expected outcomes, demonstrating that a lookahead-based approach can significantly improve performance compared to simpler heuristic strategies. This research aims to enhance AI decision-making in real-time environments by refining existing methodologies and exploring their applications in broader AI fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem being solved is Pac-Man’s playing strategy in search of achieving the highest number of points while minimizing the number of lives lost. The Pac-Man environment is a dynamic one in which  decisions are made based on walls, current position, position of the ghosts, position of power ups (fruits) and point pellets. The problem is quantifiable as we can get a score number and number of lives, it is measurable as its success is also determined based on these quantifiable measures even including level number, or survival time. It is also replicable as (at least in our implementation) the game can be played in either identical conditions or varied similar ones. \n",
    "\n",
    "One way we can provide a solution for this problem is through reinforcement learning, where a reward based feedback (one that naturally occurs in the game as you consume pellets) helps improve decision making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "To solve the problem of optimizing Pac-Man’s playing strategy, we propose using Reinforcement Learning (RL) as the primary approach. Specifically, we will implement a Deep Q-Network (DQN), which is an extension of Q-learning that incorporates deep neural networks to approximate the optimal Q-values for state-action pairs. Our approach will enable the AI agent to learn an optimal policy by interacting with the Pac-Man environment and maximizing cumulative rewards.\n",
    "\n",
    "We propose using Deep Q-Networks (DQN) to optimize Pac-Man’s strategy through Reinforcement Learning (RL). The AI agent will learn an optimal policy by interacting with the game environment and maximizing cumulative rewards.\n",
    "\n",
    "**Approach**\n",
    "\n",
    "1. State Representation: Encodes Pac-Man’s position, ghost locations, power-ups, pellets, and score.\n",
    "2. Action Space: Four moves—Up, Down, Left, Right.\n",
    "3. Reward Function:\n",
    "    - +10 for pellets, +50 for power-ups, +200 for eating ghosts.\n",
    "    - -100 for getting caught, +500 for level completion.\n",
    "    - Small penalties to encourage efficiency.\n",
    "\n",
    "**Model Implementation**\n",
    "\n",
    "- DQN with Experience Replay to store and learn from past actions.\n",
    "- Epsilon-greedy strategy to balance exploration and exploitation.\n",
    "- Bellman Equation updates Q-values for optimal decision-making.\n",
    "\n",
    "**Benchmark & Evaluation**\n",
    "\n",
    "- Baseline Heuristic Model: Moves toward pellets, avoids ghosts, prioritizes power-ups.\n",
    "- Performance Metrics:\n",
    "    - Cumulative score, levels completed, survival time, and ghosts eaten per power-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "To evaluate the performance of our model, we will count the cumulative reward of the agent in each game. We will also measure how well the agent avoids the ghosts, measured by both whether or not the agent gets eaten by a ghost, and if so, how long they were able to avoid the ghosts. To evaluate how well the model is learning the game, we will count the cumulative reward of each episode, and analyze whether or not the cumulative reward is increasing across episodes. For the ability to avoid ghosts, we will similarly track the amount of time the agent gets caught by a ghost, if at all, across many episodes and see if that time decreases. We will likely also keep a count of how often the agent gets caught by a ghost, regardless of the exact time, to see if the frequency of getting caught decreases across more and more episodes. For both metrics, we will likely create data visualizations to help visualize whether the agent is effectively learning the game. \n",
    "\n",
    "Additionally, we may create a data model such as a linear regression model or a rolling mean to quantify how well the model is learning, and possibly compare those metrics with metrics from other models. A rolling mean is derived by taking the average over a specific number of past trials (rather than all of the trials), also called a “window size.” Taking a rolling window size for the average helps to deconflate the impact of outliers, as well as view progress better than an overall average. We may also use linear regression, which finds the best fitting straight line, to get a metric for the rate of learning, which could allow us to easily compare how effectively our model is learning compared to other models. Some possible issues with this method is that reinforcement learning might not be well modeled with a linear regression model, so it would just be used to give us a rough metric for how our model is stacking up to others, and whether or not it is learning at all. \n",
    "\n",
    "**Derivation of rolling mean and mathematical representation:**\n",
    "\n",
    "The rolling average can be represented by the following equation:\n",
    "\n",
    "$$\n",
    "R_{\\text{smooth},t} = \\frac{1}{N} \\sum_{i=t-N+1}^{t} R_i\n",
    "$$\n",
    "\n",
    "Where N is the window size, or number of past samples that we want to include in our average, and Ri represents our data points. This method works by taking the average at every time step, using the last N number of samples. \n",
    "\n",
    "\n",
    "**Derivation of linear regression and mathematical representation:**\n",
    "\n",
    "Linear regression finds the best fitting straight line through a data set, and can be represented with the equation:\n",
    "\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "Where x is the input data, y is the estimated output, m is the slope of the line, and b is the y-intercept. \n",
    "\n",
    "\n",
    "**Additionally, the fit of the line to the data can be measured by the sum of square errors:**\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where yi is the predicted value. This equation measures the distance between the data points to the line, which helps evaluate how well the line is fitting to the data. A linear regression model takes the slope that results in the smallest SSE, signifying the least possible distance between data points and its graphical representation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "For our project, the main possible areas of ethical issues lie in analysis and modeling. For example, we want to be aware that our model is not relying on heuristics or proxies that are unfairly discriminatory. Honest and unbiased analysis is also something we will need to consider, since different forms of analysis may lead us to different conclusions, which could result in misjudging the effectiveness of one model over another. \n",
    "There are also potential ethical and privacy issues if we decide to compare our model with data from real people playing this game. In this case, we would probably be using a dataset off of the internet, and in cases like these, there is always the possibility that a person did not fully understand the ways in which their data was going to be used, and did not read the fine print of the consent form. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
